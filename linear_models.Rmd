---
title: "Linear models"
output:
  html_document: 
    toc: true
    toc_float: true
---

Linear regression models are fundamental in statistics and data science. When seeking to understand how covariates are associated with outcomes, linear models are among the first, best options. Although other regression approaches are possible, the flexibility and  interpretability and of linear models make them essential. 

This is the first module in the [Linear Models](topic_linear_models.html) topic; the relevant slack channel is [here](https://p8105-fall2018.slack.com/messages/CCAAE9AAU).

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

library(tidyverse)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Some slides

<script async class="speakerdeck-embed" data-id="285289b17d194a4282d53f1800d37199" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
<div style="margin-bottom:5px"> <strong> <a href="https://speakerdeck.com/jeffgoldsmith/p8105-simulation" title="Simulation and Bootstrapping" target="_blank">Simulation</a> </strong> from <strong><a href="https://speakerdeck.com/jeffgoldsmith" target="_blank">Jeff Goldsmith</a></strong>. </div><br>


## Example

I'll write code for today's content in a new R Markdown document called `linear_models.Rmd` in a `linear_models` directory / repo. The code chunk below loads the usual packages and sets a seed for reproducibility.

```{r}
library(tidyverse)
library(p8105.datasets)

set.seed(1)
```


### Model fitting

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(boro = neighbourhood_group,
         neighborhood = neighbourhood) %>% 
  filter(boro != "Staten Island") %>% 
  select(price, stars, boro, neighborhood, room_type)
```

```{r}
fit = lm(price ~ stars + boro, data = nyc_airbnb)
summary(fit)
```

```{r}
nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(boro = fct_infreq(boro),
         room_type = fct_infreq(room_type))

fit = lm(price ~ stars + boro, data = nyc_airbnb)
summary(fit)
```


### Tidying output

```{r}
fit %>% 
  broom::glance()
```

```{r}
fit %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "^boro", "Boro: ")) %>% 
  knitr::kable(digits = 3)
```

`broom::tidy` works with lots of things...


### Diagnostics

```{r}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  modelr::add_predictions(fit)
```

```{r}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = boro, y = resid)) + geom_violin()

nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = stars, y = resid)) + geom_point()
```

Huge outliers / skewed distribution!! Should we exclude? Transform ...? Use a "robust" regression???

For now I'm gonna ignore this, although it is definitely a problem. 


### Hypothesis testing

Testing a single coef is pretty easy -- it's right there in the output.

Comparing nested models 

```{r}
fit_null = lm(price ~ stars + boro, data = nyc_airbnb)
fit_alt = lm(price ~ stars + boro + room_type, data = nyc_airbnb)

anova(fit_null, fit_alt) %>% 
  broom::tidy()
```

This works for nested models only



### Nesting data

(not nesting models -- sorry)

This is confusing

```{r}
nyc_airbnb %>% 
  lm(price ~ stars * boro + room_type * boro, data = .)
```

This is clearer

```{r}
nest_lm_res =
  nyc_airbnb %>% 
  group_by(boro) %>% 
  nest() %>% 
  mutate(models = map(data, ~lm(price ~ stars + room_type, data = .x)),
         models = map(models, broom::tidy)) %>% 
  select(-data) %>% 
  unnest()

nest_lm_res %>% 
  select(boro, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  spread(key = term, value = estimate) %>% 
  knitr::kable(digits = 3)
```

tradeoff: nesting gives you separate models, which is an easy-to-interpret way to examine effect modification in an exploratory way. But it *doesn't* give a way to test. 

Even more extreme:

```{r}
manhattan_airbnb =
  nyc_airbnb %>% 
  filter(boro == "Manhattan")

manhattan_airbnb %>% 
  count(neighborhood)

manhattan_nest_lm_res =
  manhattan_airbnb %>% 
  group_by(neighborhood) %>% 
  nest() %>% 
  mutate(models = map(data, ~lm(price ~ stars + room_type, data = .x)),
         models = map(models, broom::tidy)) %>% 
  select(-data) %>% 
  unnest()

manhattan_nest_lm_res %>% 
  mutate(term = fct_inorder(term)) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term, nrow = 2, ncol = 2) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 5))
```


Really the way to fit this is a mixed model -- random intercepts and slopes for each neighborhood

```{r}
manhattan_airbnb %>% 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = .) %>% 
  broom::tidy()
```


### Binary outcomes

Linear models are appropriate for outcomes that follow a continuous distribution, but binary outcomes are common. In these cases, logistic regression is a useful analytic framework. 

The _Washington Post_ has gathered data on homicides in 50 large U.S. cities and made the data available through a[GitHub repository](https://github.com/washingtonpost/data-homicides); the final CSV is [here](data/homicide-data.csv). You can read their accompanying article [here](https://www.washingtonpost.com/graphics/2018/investigations/where-murders-go-unsolved/). We'll use data on unresolved murders in Baltimore, MD to illustrate logistic regression in R. The code below imports, cleans, and generally wrangles the data for analysis.

```{r}
baltimore_df = 
  read_csv("data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(resolved = as.numeric(disposition == "Closed by arrest"),
         victim_age = as.numeric(victim_age),
         victim_race = fct_relevel(victim_race, "White"))
```

Using these data, we can fit a logistic regression for the binary "resolved" outcome and victim demographics as predictors. This uses the `glm` function with the family specified to account for the non-Gaussian outcome distribution.

```{r}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```

Many of the same tools we used to work with `lm` fits can be used for `glm` fits. The table below summaries the coefficients from the model fit; because logistic model estimates are log odds ratios, we include a step to compute odds ratios as well.

```{r}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

We can also compute fitted values; similarly to the estimates in the model summary, these are expressed as log odds and can be transformed to produce probabilities for each subject.

```{r}
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))
```


## Other materials

* This page touches on ideas that arise in several chapters on modeling in R for Data Science. These tend to assume that this is your first exposure to linear models but good reading:
    * [Intro to modeling](https://r4ds.had.co.nz/model-intro.html)
    * [Basics](https://r4ds.had.co.nz/model-basics.html)
    * [Many models](https://r4ds.had.co.nz/many-models.html)
* The `modelr` package also has a [website](https://modelr.tidyverse.org)

The code that I produced working examples in lecture is [here](https://github.com/P8105/linear_models).
